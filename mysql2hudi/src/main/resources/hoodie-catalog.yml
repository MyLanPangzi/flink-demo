hoodie:
  hudi: # catalog
    test: # db
      test: # table config
        path: file:///Users/xiebo/IdeaProjects/flink-demo/hudi/test
        partition.default_name: default
        changelog.enabled: false
        metadata.enabled: false
        metadata.compaction.delta_commits: 10
        index.bootstrap.enabled: false
        index.state.ttl: 0.0
        index.global.enabled: true
        index.partition.regex: .*
        read.tasks: 4
        source.avro-schema.path: null
        source.avro-schema: null
        hoodie.datasource.query.type: snapshot
        hoodie.datasource.merge.type: payload_combine
        read.utc-timezone: true
        read.streaming.enabled: false
        read.streaming.check-interval: 60
        read.streaming.skip_compaction: false
        read.start-commit: null
        read.end-commit: null
        hoodie.table.name: null
        table.type: COPY_ON_WRITE
        write.insert.cluster: false
        write.operation: upsert
        write.precombine.field: ts
        write.payload.class: org.apache.hudi.common.model.OverwriteWithLatestAvroPayload
        write.precombine: false
        write.retry.times: 3
        write.retry.interval.ms: 2000
        write.ignore.failed: true
        hoodie.datasource.write.recordkey.field: uuid
        hoodie.datasource.write.partitionpath.field:
        hoodie.datasource.write.partitionpath.urlencode: false
        hoodie.datasource.write.hive_style_partitioning: false
        hoodie.datasource.write.keygenerator.class:
        hoodie.datasource.write.keygenerator.type: SIMPLE
        write.partition.format: null
        write.index_bootstrap.tasks: null
        write.bucket_assign.tasks: null
        write.tasks: 4
        write.task.max.size: 1024.0
        write.rate.limit: 0
        write.batch.size: 256.0
        write.log_block.size: 128
        write.log.max.size: 1024
        write.parquet.block.size: 120
        write.parquet.max.file.size: 120
        write.parquet.page.size: 1
        write.merge.max_memory: 100
        write.commit.ack.timeout: -1
        write.bulk_insert.shuffle_by_partition: true
        write.bulk_insert.sort_by_partition: true
        write.sort.memory: 128
        compaction.schedule.enabled: true
        compaction.async.enabled: true
        compaction.tasks: 4
        compaction.trigger.strategy: num_commits
        compaction.delta_commits: 5
        compaction.delta_seconds: 3600
        compaction.timeout.seconds: 1200
        compaction.max_memory: 100
        compaction.target_io: 512000
        clean.async.enabled: true
        clean.retain_commits: 10
        archive.max_commits: 30
        archive.min_commits: 20
        hive_sync.enable: false
        hive_sync.db: default
        hive_sync.table: unknown
        hive_sync.file_format: PARQUET
        hive_sync.mode: jdbc
        hive_sync.username: hive
        hive_sync.password: hive
        hive_sync.jdbc_url: jdbc:hive2://localhost:10000
        hive_sync.metastore.uris:
        hive_sync.partition_fields:
        hive_sync.partition_extractor_class: org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor
        hive_sync.assume_date_partitioning: false
        hive_sync.use_jdbc: true
        hive_sync.auto_create_db: true
        hive_sync.ignore_exceptions: false
        hive_sync.skip_ro_suffix: false
        hive_sync.support_timestamp: false
    t1: # db
      test: # table config
        path: file:///Users/xiebo/IdeaProjects/flink-demo/hudi/t1
        partition.default_name: default
        changelog.enabled: false
        metadata.enabled: false
        metadata.compaction.delta_commits: 10
        index.bootstrap.enabled: false
        index.state.ttl: 0.0
        index.global.enabled: true
        index.partition.regex: .*
        read.tasks: 4
        source.avro-schema.path: null
        source.avro-schema: null
        hoodie.datasource.query.type: snapshot
        hoodie.datasource.merge.type: payload_combine
        read.utc-timezone: true
        read.streaming.enabled: false
        read.streaming.check-interval: 60
        read.streaming.skip_compaction: false
        read.start-commit: null
        read.end-commit: null
        hoodie.table.name: null
        table.type: COPY_ON_WRITE
        write.insert.cluster: false
        write.operation: upsert
        write.precombine.field: ts
        write.payload.class: org.apache.hudi.common.model.OverwriteWithLatestAvroPayload
        write.precombine: false
        write.retry.times: 3
        write.retry.interval.ms: 2000
        write.ignore.failed: true
        hoodie.datasource.write.recordkey.field: uuid
        hoodie.datasource.write.partitionpath.field:
        hoodie.datasource.write.partitionpath.urlencode: false
        hoodie.datasource.write.hive_style_partitioning: false
        hoodie.datasource.write.keygenerator.class:
        hoodie.datasource.write.keygenerator.type: SIMPLE
        write.partition.format: null
        write.index_bootstrap.tasks: null
        write.bucket_assign.tasks: null
        write.tasks: 4
        write.task.max.size: 1024.0
        write.rate.limit: 0
        write.batch.size: 256.0
        write.log_block.size: 128
        write.log.max.size: 1024
        write.parquet.block.size: 120
        write.parquet.max.file.size: 120
        write.parquet.page.size: 1
        write.merge.max_memory: 100
        write.commit.ack.timeout: -1
        write.bulk_insert.shuffle_by_partition: true
        write.bulk_insert.sort_by_partition: true
        write.sort.memory: 128
        compaction.schedule.enabled: true
        compaction.async.enabled: true
        compaction.tasks: 4
        compaction.trigger.strategy: num_commits
        compaction.delta_commits: 5
        compaction.delta_seconds: 3600
        compaction.timeout.seconds: 1200
        compaction.max_memory: 100
        compaction.target_io: 512000
        clean.async.enabled: true
        clean.retain_commits: 10
        archive.max_commits: 30
        archive.min_commits: 20
        hive_sync.enable: false
        hive_sync.db: default
        hive_sync.table: unknown
        hive_sync.file_format: PARQUET
        hive_sync.mode: jdbc
        hive_sync.username: hive
        hive_sync.password: hive
        hive_sync.jdbc_url: jdbc:hive2://localhost:10000
        hive_sync.metastore.uris:
        hive_sync.partition_fields:
        hive_sync.partition_extractor_class: org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor
        hive_sync.assume_date_partitioning: false
        hive_sync.use_jdbc: true
        hive_sync.auto_create_db: true
        hive_sync.ignore_exceptions: false
        hive_sync.skip_ro_suffix: false
        hive_sync.support_timestamp: false